# -*- coding: utf-8 -*-
"""question 3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CWD1u0niRWx7W_6wXONqjI5Ow8ztrrBP
"""

import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras.datasets import mnist
from tensorflow.keras.utils import to_categorical

def create_dense_network(input_shape, num_classes, hidden_layers, activation_functions):
    """
    Creates a dense feedforward neural network.

    :param input_shape: Tuple specifying the shape of input data.
    :param num_classes: Number of output classes.
    :param hidden_layers: List of integers, where each integer specifies the number of neurons in that layer.
    :param activation_functions: List of activation functions to use in each hidden layer.
    :return: A compiled Keras model.
    """
    model = models.Sequential()
    model.add(layers.Flatten(input_shape=input_shape))  # Flatten the input images

    # Add the hidden layers
    for neurons, activation in zip(hidden_layers, activation_functions):
        model.add(layers.Dense(neurons, activation=activation))

    # Output layer
    model.add(layers.Dense(num_classes, activation='softmax'))

    # Compile the model
    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

    return model

# Example usage
input_shape = (28, 28)  # Example for MNIST
num_classes = 10  # Example for MNIST
hidden_layers = [512, 256]  # Two hidden layers with 512 and 256 neurons
activation_functions = ['relu', 'relu']  # Using ReLU activation for both hidden layers

model = create_dense_network(input_shape, num_classes, hidden_layers, activation_functions)
print(model.summary())

import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras.datasets import mnist
from tensorflow.keras.utils import to_categorical

def create_dense_network(input_shape, num_classes, hidden_layers, activation_functions):
    """
    Creates a dense feedforward neural network.

    :param input_shape: Tuple specifying the shape of input data.
    :param num_classes: Number of output classes.
    :param hidden_layers: List of integers, where each integer specifies the number of neurons in that layer.
    :param activation_functions: List of activation functions to use in each hidden layer.
    :return: A compiled Keras model.
    """
    model = models.Sequential()
    model.add(layers.Flatten(input_shape=input_shape))  # Flatten the input images

    # Add the hidden layers
    for neurons, activation in zip(hidden_layers, activation_functions):
        model.add(layers.Dense(neurons, activation=activation))

    # Output layer
    model.add(layers.Dense(num_classes, activation='softmax'))

    # Compile the model
    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

    return model

# Load and preprocess the MNIST dataset
(train_images, train_labels), (test_images, test_labels) = mnist.load_data()
train_images = train_images.astype('float32') / 255  # Normalize the images to [0, 1]
test_images = test_images.astype('float32') / 255  # Normalize the images to [0, 1]
train_labels = to_categorical(train_labels, 10)  # Convert labels to one-hot encoding
test_labels = to_categorical(test_labels, 10)  # Convert labels to one-hot encoding

# Define the network parameters
input_shape = (28, 28)  # MNIST images are 28x28
num_classes = 10  # Digits 0-9
hidden_layers = [512, 256]  # Example: two hidden layers with 512 and 256 neurons
activation_functions = ['relu', 'relu']  # ReLU activation for both hidden layers

# Create and train the model
model = create_dense_network(input_shape, num_classes, hidden_layers, activation_functions)
model.fit(train_images, train_labels, epochs=10, batch_size=128, validation_split=0.2)

# Evaluate the model on test data
test_loss, test_acc = model.evaluate(test_images, test_labels)
print(f"Test accuracy: {test_acc}")

pip install keras-tuner

import tensorflow as tf
from tensorflow.keras.datasets import mnist
from tensorflow.keras.utils import to_categorical
from kerastuner import HyperModel
from kerastuner.tuners import RandomSearch

class MyHyperModel(HyperModel):
    def __init__(self, input_shape, num_classes):
        self.input_shape = input_shape
        self.num_classes = num_classes

    def build(self, hp):
        model = tf.keras.Sequential()
        model.add(tf.keras.layers.Flatten(input_shape=self.input_shape))

        # Tuning the number of units in the first Dense layer
        # Choose an optimal value between 32-512
        for i in range(hp.Int('num_layers', 1, 3)):
            model.add(tf.keras.layers.Dense(units=hp.Int('units_' + str(i),
                                                        min_value=32,
                                                        max_value=512,
                                                        step=32),
                                            activation='relu'))

        model.add(tf.keras.layers.Dense(self.num_classes, activation='softmax'))

        # Tune the learning rate for the optimizer
        # Choose an optimal value from 0.01, 0.001, or 0.0001
        hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])

        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=hp_learning_rate),
                      loss='categorical_crossentropy',
                      metrics=['accuracy'])

        return model

# Load and preprocess the MNIST dataset
(train_images, train_labels), (test_images, test_labels) = mnist.load_data()
train_images = train_images / 255.0
test_images = test_images / 255.0
train_labels = to_categorical(train_labels)
test_labels = to_categorical(test_labels)

# Define the hypermodel
input_shape = (28, 28)
num_classes = 10
hypermodel = MyHyperModel(input_shape, num_classes)

# Initialize the tuner
tuner = RandomSearch(
    hypermodel,
    objective='val_accuracy',
    max_trials=10,
    executions_per_trial=2,
    directory='my_dir',
    project_name='helloworld')

# Display search space summary
tuner.search_space_summary()

# Perform hyperparameter tuning
tuner.search(train_images, train_labels, epochs=10, validation_split=0.2)

# Get the optimal hyperparameters
best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]

print(f"""
The hyperparameter search is complete. The optimal number of units in the first densely-connected
layer is {best_hps.get('units')} and the optimal learning rate for the optimizer
is {best_hps.get('learning_rate')}.
""")

# Build the model with the optimal hyperparameters and train it on the data
model = tuner.hypermodel.build(best_hps)
history = model.fit(train_images, train_labels, epochs=10, validation_split=0.2)

# -*- coding: utf-8 -*-
"""Untitled11.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LukXlPWlKsganRwb7UrzkKEzyq2CoulB
"""

import numpy as np
import tensorflow as tf
from tensorflow.keras import layers, models

# Define the architecture of the neural network
def build_model(input_shape, num_hidden_layers, num_neurons_per_layer, activation):
    model = models.Sequential()
    model.add(layers.Flatten(input_shape=input_shape))  # Flatten the input images
    for _ in range(num_hidden_layers):
        model.add(layers.Dense(num_neurons_per_layer, activation=activation))
    model.add(layers.Dense(1, activation='sigmoid'))  # Output layer for binary classification
    return model

# Preprocess input images
def preprocess_images(X_train, X_test):
    X_train_processed = X_train.astype('float32') / 255.0  # Normalize pixel values between 0 and 1
    X_test_processed = X_test.astype('float32') / 255.0
    return X_train_processed, X_test_processed

# Load and preprocess dataset
(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()
X_train, X_test = preprocess_images(X_train, X_test)

# Specify hyperparameters
input_shape = X_train[0].shape
num_hidden_layers = 2
num_neurons_per_layer = 128
activation = 'relu'

# Build and compile the model
model = build_model(input_shape, num_hidden_layers, num_neurons_per_layer, activation)
model.compile(optimizer='adam',
              loss='binary_crossentropy',  # For binary classification
              metrics=['accuracy'])

# Train the model
history = model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test))

# Evaluate the model
test_loss, test_acc = model.evaluate(X_test, y_test)
print("Test accuracy:", test_acc)

import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras.datasets import mnist
from tensorflow.keras.utils import to_categorical
from sklearn.model_selection import train_test_split

# Load and preprocess the MNIST dataset
(train_images, train_labels), (test_images, test_labels) = mnist.load_data()
train_images = train_images.astype('float32') / 255  # Normalize the images to [0, 1]
test_images = test_images.astype('float32') / 255  # Normalize the images to [0, 1]
train_labels = to_categorical(train_labels, 10)  # Convert labels to one-hot encoding
test_labels = to_categorical(test_labels, 10)  # Convert labels to one-hot encoding

# Split original training data into new training and validation sets
train_images, val_images, train_labels, val_labels = train_test_split(
    train_images, train_labels, test_size=0.2, random_state=42)

def create_and_train_model(input_shape, num_classes, hidden_layers, activation_functions, batch_size, epochs):
    model = models.Sequential()
    model.add(layers.Flatten(input_shape=input_shape))  # Flatten the input images

    # Add the hidden layers
    for neurons, activation in zip(hidden_layers, activation_functions):
        model.add(layers.Dense(neurons, activation=activation))

    # Output layer
    model.add(layers.Dense(num_classes, activation='softmax'))

    # Compile the model
    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

    # Train the model
    history = model.fit(train_images, train_labels, epochs=epochs, batch_size=batch_size,
                        validation_data=(val_images, val_labels), verbose=0)
    return history

# Hyperparameter tuning
batch_sizes = [128, 256]
epochs = [10, 20]
hidden_layer_configs = [
    ([512, 256], ['relu', 'relu']),
    ([256, 128], ['relu', 'relu'])
]

best_val_accuracy = 0
best_config = {}

for batch_size in batch_sizes:
    for epoch in epochs:
        for hidden_layers, activation_functions in hidden_layer_configs:
            history = create_and_train_model((28, 28), 10, hidden_layers, activation_functions, batch_size, epoch)
            val_accuracy = max(history.history['val_accuracy'])  # Get the best validation accuracy

            if val_accuracy > best_val_accuracy:
                best_val_accuracy = val_accuracy
                best_config = {
                    'batch_size': batch_size,
                    'epochs': epoch,
                    'hidden_layers': hidden_layers,
                    'activation_functions': activation_functions,
                    'val_accuracy': val_accuracy
                }

print(f"Best Validation Accuracy: {best_val_accuracy}")
print(f"Best Configuration: {best_config}")

# Train the final model with the best configuration
final_model = create_and_train_model((28, 28), 10, best_config['hidden_layers'],
                                     best_config['activation_functions'], best_config['batch_size'], best_config['epochs'])

# Evaluate the model on test data
test_loss, test_acc = final_model.evaluate(test_images, test_labels, verbose=0)
print(f"Test accuracy with best configuration: {test_acc}")

import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras.datasets import mnist
from tensorflow.keras.utils import to_categorical
import numpy as np

def create_dense_network(input_shape, num_classes, hidden_layers, activation_functions):
    model = models.Sequential()
    model.add(layers.Flatten(input_shape=input_shape))
    for neurons, activation in zip(hidden_layers, activation_functions):
        model.add(layers.Dense(neurons, activation=activation))
    model.add(layers.Dense(num_classes, activation='softmax'))
    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
    return model

# Load and preprocess the MNIST dataset
(train_images, train_labels), (test_images, test_labels) = mnist.load_data()
train_images = train_images.astype('float32') / 255
test_images = test_images.astype('float32') / 255
train_labels = to_categorical(train_labels, 10)
test_labels = to_categorical(test_labels, 10)

# Hyperparameter sets for tuning
hyperparameters = [
    {'batch_size': 128, 'epochs': 10, 'hidden_layers': [512], 'activation_functions': ['relu']},
    {'batch_size': 128, 'epochs': 20, 'hidden_layers': [512, 256], 'activation_functions': ['relu', 'relu']}
]

# Placeholder for the best model's validation accuracy and configuration
best_val_accuracy = 0
best_config = None
final_model = None

for config in hyperparameters:
    model = create_dense_network(input_shape=(28, 28), num_classes=10,
                                 hidden_layers=config['hidden_layers'],
                                 activation_functions=config['activation_functions'])
    history = model.fit(train_images, train_labels,
                        epochs=config['epochs'],
                        batch_size=config['batch_size'],
                        validation_split=0.2,
                        verbose=0)

    val_accuracy = max(history.history['val_accuracy'])
    if val_accuracy > best_val_accuracy:
        best_val_accuracy = val_accuracy
        best_config = config
        final_model = model  # Update the final_model with the current best model

print(f"Best Validation Accuracy: {best_val_accuracy}")
print(f"Best Configuration: {best_config}")

# Correctly evaluate the final model on the test data
if final_model is not None:
    test_loss, test_acc = final_model.evaluate(test_images, test_labels, verbose=0)
    print(f"Test accuracy with best configuration: {test_acc}")
else:
    print("No model was trained.")

